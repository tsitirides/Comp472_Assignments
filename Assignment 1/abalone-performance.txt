--------------------------------------------------
Model: Base-DT
(A)
Best Hyperparameters: None
(B)
Confusion Matrix:
[[138  41 154]
 [ 56 206  68]
 [160  61 161]]
(C)
Classification Report:
              precision    recall  f1-score   support

           0       0.39      0.41      0.40       333
           1       0.67      0.62      0.65       330
           2       0.42      0.42      0.42       382

    accuracy                           0.48      1045
   macro avg       0.49      0.49      0.49      1045
weighted avg       0.49      0.48      0.49      1045
(D)
Accuracy: 0.4833
Macro-average F1: 0.4895
Weighted-average F1: 0.4858

--------------------------------------------------
Model: Top-DT
(A)
Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 2}
(B)
Confusion Matrix:
[[ 91  71 171]
 [  6 265  59]
 [ 82  89 211]]
(C)
Classification Report:
              precision    recall  f1-score   support

           0       0.51      0.27      0.36       333
           1       0.62      0.80      0.70       330
           2       0.48      0.55      0.51       382

    accuracy                           0.54      1045
   macro avg       0.54      0.54      0.52      1045
weighted avg       0.53      0.54      0.52      1045
(D)
Accuracy: 0.5426
Macro-average F1: 0.5234
Weighted-average F1: 0.5224

--------------------------------------------------
Model: Base-MLP
(A)
Best Hyperparameters: None
(B)
Confusion Matrix:
[[  0  33 300]
 [  0 211 119]
 [  0  55 327]]
(C)
Classification Report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       333
           1       0.71      0.64      0.67       330
           2       0.44      0.86      0.58       382

    accuracy                           0.51      1045
   macro avg       0.38      0.50      0.42      1045
weighted avg       0.38      0.51      0.42      1045
(D)
Accuracy: 0.5148
Macro-average F1: 0.4169
Weighted-average F1: 0.4238

--------------------------------------------------
Model: Top-MLP
(A)
Best Hyperparameters: {'activation': 'tanh', 'hidden_layer_sizes': (30, 50), 'solver': 'adam'}
(B)
Confusion Matrix:
[[ 13  39 281]
 [  1 246  83]
 [ 20  67 295]]
(C)
Classification Report:
              precision    recall  f1-score   support

           0       0.38      0.04      0.07       333
           1       0.70      0.75      0.72       330
           2       0.45      0.77      0.57       382

    accuracy                           0.53      1045
   macro avg       0.51      0.52      0.45      1045
weighted avg       0.51      0.53      0.46      1045
(D)
Accuracy: 0.5301
Macro-average F1: 0.4530
Weighted-average F1: 0.4576

--------------------------------------------------
Model: Base-DT
(A)
Best Hyperparameters: None
(B)
Confusion Matrix:
[[137  45 151]
 [ 54 204  72]
 [150  66 166]]
(C)
Classification Report:
              precision    recall  f1-score   support

           0       0.40      0.41      0.41       333
           1       0.65      0.62      0.63       330
           2       0.43      0.43      0.43       382

    accuracy                           0.49      1045
   macro avg       0.49      0.49      0.49      1045
weighted avg       0.49      0.49      0.49      1045
(D)
Accuracy: 0.4852
Macro-average F1: 0.4899
Weighted-average F1: 0.4867

--------------------------------------------------
Model: Top-DT
(A)
Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 2}
(B)
Confusion Matrix:
[[ 91  71 171]
 [  6 265  59]
 [ 82  89 211]]
(C)
Classification Report:
              precision    recall  f1-score   support

           0       0.51      0.27      0.36       333
           1       0.62      0.80      0.70       330
           2       0.48      0.55      0.51       382

    accuracy                           0.54      1045
   macro avg       0.54      0.54      0.52      1045
weighted avg       0.53      0.54      0.52      1045
(D)
Accuracy: 0.5426
Macro-average F1: 0.5234
Weighted-average F1: 0.5224

--------------------------------------------------
Model: Base-MLP
(A)
Best Hyperparameters: None
(B)
Confusion Matrix:
[[  0  53 280]
 [  0 219 111]
 [  0  81 301]]
(C)
Classification Report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       333
           1       0.62      0.66      0.64       330
           2       0.43      0.79      0.56       382

    accuracy                           0.50      1045
   macro avg       0.35      0.48      0.40      1045
weighted avg       0.35      0.50      0.41      1045
(D)
Accuracy: 0.4976
Macro-average F1: 0.4006
Weighted-average F1: 0.4074

--------------------------------------------------
Model: Top-MLP
(A)
Best Hyperparameters: {'activation': 'tanh', 'hidden_layer_sizes': (30, 50), 'solver': 'adam'}
(B)
Confusion Matrix:
[[ 62  34 237]
 [  8 246  76]
 [ 63  68 251]]
(C)
Classification Report:
              precision    recall  f1-score   support

           0       0.47      0.19      0.27       333
           1       0.71      0.75      0.73       330
           2       0.45      0.66      0.53       382

    accuracy                           0.53      1045
   macro avg       0.54      0.53      0.51      1045
weighted avg       0.53      0.53      0.51      1045
(D)
Accuracy: 0.5349
Macro-average F1: 0.5075
Weighted-average F1: 0.5079

--------------------------------------------------
Model: Base-DT
(A)
Best Hyperparameters: None
(B)
Confusion Matrix:
[[137  44 152]
 [ 57 201  72]
 [168  62 152]]
(C)
Classification Report:
              precision    recall  f1-score   support

           0       0.38      0.41      0.39       333
           1       0.65      0.61      0.63       330
           2       0.40      0.40      0.40       382

    accuracy                           0.47      1045
   macro avg       0.48      0.47      0.48      1045
weighted avg       0.48      0.47      0.47      1045
(D)
Accuracy: 0.4689
Macro-average F1: 0.4755
Weighted-average F1: 0.4715

--------------------------------------------------
Model: Top-DT
(A)
Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 2}
(B)
Confusion Matrix:
[[ 91  71 171]
 [  6 265  59]
 [ 82  89 211]]
(C)
Classification Report:
              precision    recall  f1-score   support

           0       0.51      0.27      0.36       333
           1       0.62      0.80      0.70       330
           2       0.48      0.55      0.51       382

    accuracy                           0.54      1045
   macro avg       0.54      0.54      0.52      1045
weighted avg       0.53      0.54      0.52      1045
(D)
Accuracy: 0.5426
Macro-average F1: 0.5234
Weighted-average F1: 0.5224

--------------------------------------------------
Model: Base-MLP
(A)
Best Hyperparameters: None
(B)
Confusion Matrix:
[[  0  35 298]
 [  0 217 113]
 [  0  61 321]]
(C)
Classification Report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       333
           1       0.69      0.66      0.67       330
           2       0.44      0.84      0.58       382

    accuracy                           0.51      1045
   macro avg       0.38      0.50      0.42      1045
weighted avg       0.38      0.51      0.42      1045
(D)
Accuracy: 0.5148
Macro-average F1: 0.4171
Weighted-average F1: 0.4238

--------------------------------------------------
Model: Top-MLP
(A)
Best Hyperparameters: {'activation': 'tanh', 'hidden_layer_sizes': (30, 50), 'solver': 'adam'}
(B)
Confusion Matrix:
[[ 89  76 168]
 [ 15 281  34]
 [ 91  99 192]]
(C)
Classification Report:
              precision    recall  f1-score   support

           0       0.46      0.27      0.34       333
           1       0.62      0.85      0.72       330
           2       0.49      0.50      0.49       382

    accuracy                           0.54      1045
   macro avg       0.52      0.54      0.52      1045
weighted avg       0.52      0.54      0.51      1045
(D)
Accuracy: 0.5378
Macro-average F1: 0.5157
Weighted-average F1: 0.5141

